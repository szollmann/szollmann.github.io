---
---


@STRING{CVPR = {Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)}}
@STRING{ECCV = {Proc. of the European Conf. on Computer Vision (ECCV)}}
@STRING{ICCV = {Proc. of the IEEE International Conf. on Computer Vision (ICCV)}}
@STRING{THREEDV = {Proc. of the International Conf. on 3D Vision (3DV)}}
@STRING{NEURIPS = {Advances in Neural Information Processing Systems (NeurIPS)}}
@STRING{ARXIV = {arXiv.org}}
@STRING{ICLR = {Proc. of the International Conf. on Learning Representations (ICLR)}}

@inproceedings{10322162,
  author = {Schieber, Hannah and Schmid, Fritz and Mubashir-UI-Hassan and Zollmann, Stefanie and Roth, Daniel},
  booktitle = {2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
  doi = {10.1109/ISMAR-Adjunct60411.2023.00129},
  keywords = {Point cloud compression;Visualization;Three-dimens,and virtual realities,augmented},
  pages = {609--610},
  title = {A Modular Approach for 3D Reconstruction with Point Cloud Overlay},
  year = {2023},
    html = {},
    pdf = {},
    img            = {assets/img/publications/schieber2023.jpg},
}



@article{Zhang2023,
abstract = {Mixed reality technologies provide real-time and immersive experiences, which bring tremendous opportunities in entertainment, education, and enriched experiences that are not directly accessible owing to safety or cost. The research in this field has been in the spotlight in the last few years as the metaverse went viral. The recently emerging omnidirectional video streams, i.e., 360° videos, provide an affordable way to capture and present dynamic real-world scenes. In the last decade, fueled by the rapid development of artificial intelligence and computational photography technologies, the research interests in mixed reality systems using 360° videos with richer and more realistic experiences are dramatically increased to unlock the true potential of the metaverse. In this survey, we cover recent research aimed at addressing the above issues in the 360° image and video processing technologies and applications for mixed reality. The survey summarizes the contributions of the recent research and describes potential future research directions about 360° media in the field of mixed reality.},
author = {Zhang, Fanglue and Zhao, Junhong and Zhang, Yun and Zollmann, Stefanie},
doi = {10.1007/s11390-023-3210-1},
issn = {18604749},
journal = {Journal of Computer Science and Technology},
number = {3},
title = {{A Survey on 360° Images and Videos in Mixed Reality: Algorithms and Applications}},
volume = {38},
year = {2023},
  html = {},
  pdf = {},
  img            = {assets/img/publications/zhang2023.jpg},
}
@article{Baker2023,
abstract = {In augmented reality applications it is essential to know the position and orientation of the user to correctly register virtual 3D content in the user's field of view. For this purpose, visual tracking through simultaneous localization and mapping (SLAM) is often used. However, when applied to the commonly occurring situation where the users are mostly stationary, many methods presented in previous research have two key limitations. First, SLAM techniques alone do not address the problem of global localization with respect to prior models of the environment. Global localization is essential in many applications where multiple users are expected to track within a shared space, such as spectators at a sporting event. Secondly, these methods often assume significant translational movement to accurately reconstruct and track from a local model of the environment, causing challenges for many stationary applications. In this paper, we extend recent research on Spherical Localization and Tracking to support relocalization after tracking failure, as well as global localization in large shared environments, and optimize the method for operation on mobile hardware. We also evaluate various state-of-the-art localization approaches, the robustness of our visual tracking method, and demonstrate the effectiveness of our system in real-life scenarios.},
author = {Baker, Lewis and Ventura, Jonathan and Langlotz, Tobias and Gul, Shazia and Mills, Steven and Zollmann, Stefanie},
doi = {10.1007/s00371-023-02777-2},
issn = {01782789},
journal = {Visual Computer},
title = {{Localization and tracking of stationary users for augmented reality}},
year = {2023},
  html = {},
  pdf = {},
  img            = {assets/img/publications/baker2023.jpg},
}

@article{Lo2023,
abstract = {The maturity of augmented reality (AR) technology allows for expansion into real-world applications, including visualizations for on-site sports spectating. However, it is crucial to understand the factors influencing user experience in AR applications. To optimize user experience, we conducted a user study where participants adjusted parameters to determine noticeable and disruptive values of latency, registration accuracy, and jitter using a mobile indirect AR prototype that simulates a rugby stadium experience. Our findings indicate that latency has the highest disruptive impact on usersa experience, with registration accuracy following closely. Furthermore, when noticeable latency, registration accuracy, and jitter were combined, the user experience was negatively affected in a nonlinear, combinatorial manner. This suggests that addressing factors individually is necessary but not enough for successful user experiences. By understanding these factors, developers can optimize AR experiences when creating immersive AR sports experiences and other large-scale AR applications to ensure maximum enjoyment for users.},
author = {Lo, Wei Hong and Regenbrecht, Holger and Zollmann, Stefanie},
doi = {10.1109/MCG.2023.3308958},
issn = {15581756},
journal = {IEEE Computer Graphics and Applications},
number = {6},
title = {Sports Visualization in the Wild: The Impact of Technical Factors on User Experience in Augmented Reality Sports Spectating},
volume = {43},
year = {2023},
  img            = {assets/img/publications/technicalfactors.jpg}
}


@inproceedings{9974496,
author = {Lo, Wei Hong and Regenbrecht, Holger and Ens, Barrett and Zollmann, Stefanie},
booktitle = {2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
doi = {10.1109/ISMAR-Adjunct57072.2022.00125},
pages = {605--610},
title = {{A Context-aware Interface for Immersive Sports Spectating}},
year = {2022}
}
@incollection{Zollmann2022,
author = {Zollmann, Stefanie and Langlotz, Tobias and Regenbrecht, Holger and Button, Chris and Lo, Wei Hong and Mills, Steven},
booktitle = {Interactive Sports Technologies: Performance, Participation, Safety},
doi = {10.4324/9781003205111-7},
title = {{Augmented reality for sports spectating and coaching}},
year = {2022}
}
@inproceedings{Sutton2022,
abstract = {Augmented Reality has traditionally been used to display digital overlays in real environments. Many AR applications such as remote collaboration, picking tasks, or navigation require highlighting physical objects for selection or guidance. These highlights use graphical cues such as outlines and arrows. Whilst effective, they greatly contribute to visual clutter, possibly occlude scene elements, and can be problematic for long-term use. Substituting those overlays, we explore saliency modulation to accentuate objects in the real environment to guide the user's gaze. Instead of manipulating video streams, like done in perception and cognition research, we investigate saliency modulation of the real world using optical-see-through head-mounted displays. This is a new challenge, since we do not have full control over the view of the real environment. In this work we provide our specific solution to this challenge, including built prototypes and their evaluation.},
author = {Sutton, Jonathan and Langlotz, Tobias and Plopski, Alexander and Zollmann, Stefanie and Itoh, Yuta and Regenbrecht, Holger},
booktitle = {UIST 2022 - Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3526113.3545633},
title = {{Look over there! Investigating Saliency Modulation for Visual Guidance with Augmented Reality Glasses}},
year = {2022}
}
@inproceedings{Waidhofer2022,
abstract = {We investigate how real-time, 360° view synthesis can be achieved on current virtual reality hardware from a single panoramic image input. We introduce a light-weight method to automatically convert a single panoramic input into a multi-cylinder image representation that supports real-time, free-viewpoint view synthesis rendering for virtual reality. We apply an existing convolutional neural network trained on pinhole images to a cylindrical panorama with wrap padding to ensure agreement between the left and right edges. The network outputs a stack of semi-transparent panoramas at varying depths which can be easily rendered and composited with over blending. Quantitative experiments and a user study show that the method produces convincing parallax and fewer artifacts than a textured mesh representation.},
author = {Waidhofer, John and Gadgil, Richa and Dickson, Anthony and Zollmann, Stefanie and Ventura, Jonathan},
booktitle = {Proceedings - 2022 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2022},
doi = {10.1109/ISMAR55827.2022.00075},
title = {{PanoSynthVR: Toward Light-weight 360-Degree View Synthesis from a Single Panoramic Input}},
year = {2022}
}
@article{Lo2022,
abstract = {With recent technological advancements in sports broadcasting, remote spectators are presented with an enriched experience. These enriched experiences include additional content such as statistics and graphics that support game understanding. However, these technological advancements are often not accessible to on-site sports spectators. In this paper, we explore the opportunities of using situated visualization to enrich on-site sports spectating. Situated visualization techniques allow us to display information in a spatial context to its physical reference and have the potential to close the gap between on-site spectating and content access. With regards to this, we developed a framework for situated visualization focusing on sports spectating. We identified components needed for such a use case and developed two novel situated visualization approaches based on the proposed framework: (1) situated broadcast-styled visualization and (2) situated infographics. To evaluate the visualization approaches and explore user preferences, we conducted a lab study and a subsequent on-site study in a stadium.},
author = {Lo, Wei Hong and Zollmann, Stefanie and Regenbrecht, Holger},
doi = {10.1016/j.cag.2021.12.009},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
title = {{Stats on-site — Sports spectator experience through situated visualizations}},
volume = {102},
year = {2022}
}
@article{Itoh2021,
abstract = {{\textcopyright} 1995-2012 IEEE. We present computational phase-modulated eyeglasses, a see-through optical system that modulates the view of the user using phase-only spatial light modulators (PSLM). A PSLM is a programmable reflective device that can selectively retardate, or delay, the incoming light rays. As a result, a PSLM works as a computational dynamic lens device. We demonstrate our computational phase-modulated eyeglasses with either a single PSLM or dual PSLMs and show that the concept can realize various optical operations including focus correction, bi-focus, image shift, and field of view manipulation, namely optical zoom. Compared to other programmable optics, computational phase-modulated eyeglasses have the advantage in terms of its versatility. In addition, we also presents some prototypical focus-loop applications where the lens is dynamically optimized based on distances of objects observed by a scene camera. We further discuss the implementation, applications but also discuss limitations of the current prototypes and remaining issues that need to be addressed in future research.},
author = {Itoh, Y. and Langlotz, T. and Zollmann, S. and Iwai, D. and Kiyoshi, K. and Amano, T.},
doi = {10.1109/TVCG.2019.2947038},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Computational eyeglasses,LCoS,augmented vision,phase modulation,spatial light modulator},
number = {3},
title = {{Computational Phase-Modulated Eyeglasses}},
volume = {27},
year = {2021}
}
@inproceedings{Gadgil2021,
abstract = {We introduce a method to automatically convert a single panoramic input into a multi-cylinder image representation that supports real-time, free-viewpoint view synthesis for virtual reality. We apply an existing convolutional neural network trained on pinhole images to a cylindrical panorama with wrap padding to ensure agreement between the left and right edges. The network outputs a stack of semi-transparent panoramas at varying depths which can be easily rendered and composited with over blending. Initial experiments show that the method produces convincing parallax and cleaner object boundaries than a textured mesh representation.},
author = {Gadgil, Richa and John, Reesa and Zollmann, Stefanie and Ventura, Jonathan},
booktitle = {Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters, SIGGRAPH 2021},
doi = {10.1145/3450618.3469144},
isbn = {9781450383714},
title = {{PanoSynthVR: View Synthesis from A Single Input Panorama with Multi-Cylinder Images}},
year = {2021}
}
@inproceedings{HongLo2021,
abstract = {With the recent technological advancements in sports broadcasting, viewers that follow a sports game through broadcast media or online are presented with an enriched experience that includes additional content such as statistics and graphics that help to follow a game. In contrast, spectators at live sporting events often miss out on this additional content.In this paper, we explore the opportunities of using situated visualization to enrich on-site sports spectating. We developed two novel situated visualization approaches for on-site sports spectating: (1) situated broadcast-styled visualization which mimics television broadcasts and (2) situated infographics which places visual elements into the environment.},
author = {{Hong Lo}, Wei and Zollmann, Stefanie and Regenbrecht, Holger},
booktitle = {Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2021},
doi = {10.1109/VRW52623.2021.00130},
isbn = {9780738113678},
keywords = {Human-centered computing,Visual analytics,Visualization,Visualization application domains,Visualization techniques},
title = {{Who kicked the ball? Situated visualization in on-site sports spectating}},
year = {2021}
}
@inproceedings{Baker2020a,
abstract = {Hand-held capture of stereo panoramas involves spinning the camera in a roughly circular path to acquire a dense set of views of the scene. However, most existing structure-from-motion pipelines fail when trying to reconstruct such trajectories, due to the small baseline between frames. In this work, we evaluate the use of spherical structure-from-motion for reconstructing handheld stereo panorama captures. The spherical motion constraint introduces a strong regularization on the structure-from-motion process which mitigates the small-baseline problem, making it well-suited to the use case of stereo panorama capture with a handheld camera. We demonstrate the effectiveness of spherical structure-from-motion for casual capture of high-resolution stereo panoramas and validate our results with a user study.},
author = {Baker, Lewis and Mills, Steven and Zollmann, Stefanie and Ventura, Jonathan},
booktitle = {Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020},
doi = {10.1109/VR46266.2020.1581313146787},
isbn = {9781728156088},
keywords = {3D imaging,Computer vision,Human-centered computing,Image and video acquisition,Interaction paradigms,Virtual reality},
title = {{CasualStereo: Casual Capture of Stereo Panoramas with Spherical Structure-from-Motion}},
year = {2020}
}
@inproceedings{Zollmann2020,
abstract = {Thanks to the ubiquity of devices capable of recording and playing back video, the amount of video files is growing at a rapid rate. Most of us have now video recordings of major events in our lives. However, until today, these videos are captured mainly in 2D and are mostly used for screen-based video replay. Currently there is no way for watching them in more immersive environments such as on a VR headset. They are simply not optimized for playback in stereoscopic displays or even tracked Virtual Reality devices. In this work, we present CasualVRVideos, a first approach that works towards solving these issues by extracting spatial information from video footage recorded in 2D, so that it can later be played back in VR displays to increase the immersion. We focus in particular on the challenging scenario when the camera itself is not moving.},
author = {Zollmann, Stefanie and Dickson, Anthony and Ventura, Jonathan},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
doi = {10.1145/3385956.3422119},
isbn = {9781450376198},
keywords = {content creation,single view geometry,virtual reality},
title = {{CasualVRVideos: VR videos from casual stationary videos}},
year = {2020}
}
@inproceedings{Zollmann2020a,
abstract = {Thanks to the ubiquity of devices capable of recording and playing back video, the amount of video files is growing at a rapid rate. Most of us have now video recordings of major events in our lives. However, until today, these videos are captured mainly in 2D and are mostly used for screen-based video replay. Currently there is no way for watching them in more immersive environments such as on a VR headset. They are simply not optimized for playback in stereoscopic displays or even tracked Virtual Reality devices. In this work, we present CasualVRVideos, a first approach that works towards solving these issues by extracting spatial information from video footage recorded in 2D, so that it can later be played back in VR displays to increase the immersion. We focus in particular on the challenging scenario when the camera itself is not moving.},
author = {Zollmann, Stefanie and Dickson, Anthony and Ventura, Jonathan},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
doi = {10.1145/3385956.3422119},
isbn = {9781450376198},
keywords = {content creation,single view geometry,virtual reality},
title = {{CasualVRVideos: VR videos from casual stationary videos}},
year = {2020}
}
@inproceedings{Baker2020,
author = {Baker, L. and Ventura, Jonathan and Zollmann, Stefanie and Mills, Steven and Langlotz, Tobias},
booktitle = {IEEE Virtual Reality (IEEE VR)},
title = {{SPLAT: Spherical Localization and Tracking in Large Spaces}},
year = {2020}
}
@inproceedings{Ventura2020,
author = {Ventura, J. and Zollmann, S. and Stannus, S. and Billinghurst, M. and Driancourt, R.},
booktitle = {ACM SIGGRAPH 2020 Courses, SIGGRAPH 2020},
doi = {10.1145/3388769.3407543},
isbn = {9781450379724},
title = {{Understanding AR inside and out - - Part Two: Expanding out and into the world}},
year = {2020}
}
@article{Zollmann2020b,
abstract = {In recent years, the development of Augmented Reality (AR) frameworks made AR application development widely accessible to developers without AR expert background. With this development, new application fields for AR are on the rise. This comes with an increased need for visualization techniques that are suitable for a wide range of application areas. It becomes more important for a wider audience to gain a better understanding of existing AR visualization techniques. Within this work we provide a taxonomy of existing works on visualization techniques in AR. The taxonomy aims to give researchers and developers without an in-depth background in Augmented Reality the information to successively apply visualization techniques in Augmented Reality environments. We also describe required components and methods and analyze common patterns.},
author = {Zollmann, Stefanie and Grasset, Raphael and Langlotz, Tobias and Lo, Wei Hong and Mori, Shohei and Regenbrecht, Holger},
doi = {10.1109/TVCG.2020.2986247},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Augmented Reality,Augmented reality,Cameras,Data visualization,Information Visualization,Pipelines,Rendering (computer graphics),Taxonomy,Visualization},
title = {{Visualization Techniques in Augmented Reality: A Taxonomy, Methods and Patterns}},
year = {2020}
}
@inproceedings{Zollmann2019,
abstract = {Augmented Reality (AR) has gained a lot of interests recently and has been used for various applications. Most of these applications are however limited to small indoor environments. Despite the wide range of large scale application areas that could highly benefit from AR usage, until now there are rarely AR applications that target such environments. In this work, we discuss how AR can be used to enhance the experience of on-site spectators at live sport events. We investigate the challenges that come with applying AR for such a large scale environment and explore state-of-the-art technology and its suitability for an on-site AR spectator experience. We also present a concept design and explore the options to implement AR applications inside large scale environments.},
author = {Zollmann, Stefanie and Langlotz, Tobias and Loos, Moritz and Lo, Wei Hong and Baker, Lewis},
booktitle = {SIGGRAPH Asia 2019 Technical Briefs, SA 2019},
doi = {10.1145/3355088.3365162},
isbn = {9781450369459},
keywords = {Augmented reality,Concept design,Sport events},
title = {{Arspectator: Exploring augmented reality for sport events}},
year = {2019}
}
@inproceedings{Lo2019,
abstract = {{\textcopyright} 2019 Association for Computing Machinery. Traditional sports events related data have no direct spatial relationship to what spectators see when attending a live sports event. The idea of our work is to address this gap and ultimately to provide spectators insights of a sports game by embedding sports statistics into their field of view of the game using mobile Augmented Reality. Research in the area of live sport events comes with several challenges such as tracking and visualisation challenges as well as with the challenge that there are only limited opportunities to test and study new features during live games on-site. In this work, we developed a set of prototypes that allow for researching dedicated features for an AR sports spectator experience off-site in the lab before testing them live on the field.},
author = {Lo, W.H. and Zollmann, S. and Regenbrecht, H. and Loos, M.},
booktitle = {Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry},
doi = {10.1145/3359997.3365728},
isbn = {9781450370028},
keywords = {Augmented Reality,Prototype Design,Sport Events},
title = {{From lab to field: Demonstrating mixed reality prototypes for augmented sports experiences}},
year = {2019}
}
@inproceedings{Skinner2019,
abstract = {{\textcopyright} 2019 IEEE. Sports broadcasts often use pitch aligned graphics to provide additional information to the viewer. This is often achieved by using professional cameras equipped with high accuracy sensors or elaborate manual calibration techniques to measure the broadcasting cameras' position and orientation, allowing the graphics to be accurately matched to the camera view. While previous research has investigated how the camera position and orientation can be estimated for professional broadcast cameras alone, none of the previous works have targeted smartphones.In this paper, we investigate whether line pitch markings in combination with feature matching computer vision techniques can be used to estimate an on-site users position and orientation with sufficient accuracy to align augmented reality content with the pitch.},
author = {Skinner, P. and Zollmann, S.},
booktitle = {International Conference Image and Vision Computing New Zealand},
doi = {10.1109/IVCNZ48456.2019.8961006},
isbn = {9781728141879},
issn = {21512205},
title = {{Localisation for Augmented Reality at Sport Events}},
volume = {2019-Decem},
year = {2019}
}
@inproceedings{Baker2019,
abstract = {{\textcopyright} 2018 IEEE. Localization in dynamic environments is a challenging problem for Augmented Reality (AR) applications. Model-based localization methods use a prior model and an image to perform localization, and make use of the appearance of the environment. These methods cannot always be relied on, since in dynamic environments, the current appearance may be very different from the model. In this paper, we investigate SoftPOSIT, a model-based pose algorithm that does not rely on appearance. We apply line-based SoftPOSIT to a range of model types and complexities, and show that while SoftPOSIT produces promising results on basic cases, more work is needed in this area to attain useful results in complex real cases and AR applications.},
author = {Baker, L. and Zollmann, S. and Mills, S. and Langlotz, T.},
booktitle = {International Conference Image and Vision Computing New Zealand},
doi = {10.1109/IVCNZ.2018.8634761},
isbn = {9781728101255},
issn = {21512205},
keywords = {SoftPOSIT,lines,localization,pose},
title = {{Softposit for Augmented Reality in Complex Environments: Limitations and Challenges}},
volume = {2018-Novem},
year = {2019}
}
@inproceedings{Baker2019a,
abstract = {{\textcopyright} 2019 IEEE. Hand-held capture of stereo panoramas involves spinning the camera in a roughly circular path to acquire a dense set of views of the scene. However, most existing structure-from-motion pipelines fail when trying to reconstruct such trajectories, due to the small baseline between frames. In this work, we propose to use spherical structure-from-motion for reconstructing handheld stereo panorama captures. Our initial results show that spherical motion constraints are critical for reconstructing small-baseline, circular trajectories.},
author = {Baker, L. and Zollmann, S. and Ventura, J.},
booktitle = {26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings},
doi = {10.1109/VR.2019.8797794},
isbn = {9781728113777},
keywords = {Artificial intelligence,Centered computing,Computer vision,Computer vision problems,Human,Interaction paradigms,Tracking,Virtual reality},
title = {{Spherical structure-from-motion for casual capture of stereo panoramas}},
year = {2019}
}
@inproceedings{Langlotz2018,
abstract = {{\textcopyright} 2018 ACM. Prescription glasses are used by many people as a simple, and even fashionable way, to correct refractive problems of the eye. However, there are other visual impairments that cannot be treated with an optical lens in conventional glasses. In this work we present ChromaGlasses, Computational Glasses using optical head-mounted displays for compensating colour vision deficiency. Unlike prior work that required users to look at a screen in their visual periphery rather than at the environment directly, ChromaGlasses allow users to directly see the environment using a novel head-mounted displays design that analyzes the environment in real-time and changes the appearance of the environment with pixel precision to compensate the impairment of the user. In this work, we present first prototypes for ChromaGlasses and report on the results from several studies showing that ChromaGlasses are an effective method for managing colour blindness.},
author = {Langlotz, T. and Sutton, J. and Zollmann, S. and Itoh, Y. and Regenbrecht, H.},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/3173574.3173964},
isbn = {9781450356206},
keywords = {Augmented human,Augmented reality,Colour blindness,Colour vision deficiency,Computational Glasses,Head-mounted displays,Near-eye displays,Vision augmentation},
title = {{ChromaGlasses: Computational glasses for compensating colour blindness}},
volume = {2018-April},
year = {2018}
}
@inproceedings{Skinner2018,
abstract = {In Augmented Reality applications, user experience is highly dependent on the accuracy of registration between digital content and the real world. Errors in tracking and registration can arise due to inaccuracy of sensors or challenging conditions such as urban canyon effects or magnetic distortions. Indirect augmented reality is an approach that avoid these issues by using precaptured and preregistered images instead of a live video feed. However, indirect augmented reality highly depends on the availability of those preregistered images. In particular, when being used for browsing geographic information, it is important to access data in an omnipresent way. In this work, we propose an Indirect Augmented Reality browser that aims to address these availability problems by combining indirect Augmented Reality with crowd sourced precaptured street level imagery with geospatial data. We demonstrate how our indirect augmented reality browser annotates buildings and landmarks in the users' environment and investigate the feasibility by analysing the performance of such an approach. In addition, we investigate issues of visibility and legibility when labelling the environment.},
author = {Skinner, Patrick and Ventura, Jonathan and Zollmann, Stefanie},
booktitle = {Adjunct Proceedings - 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2018},
doi = {10.1109/ISMAR-Adjunct.2018.00054},
isbn = {9781538675922},
keywords = {Geographic visualization,Human computer interaction (HCI),Human-centered computing,Interaction paradigms,Mixed / augmented reality,Visualization,Visualization application domains},
title = {{Indirect Augmented Reality Browser for GIS Data}},
year = {2018}
}
@article{Jackson2018,
abstract = {{\textcopyright} 2017 The Author(s). Separate research streams have identified synchrony and arousal as two factors that might contribute to the effects of human rituals on social cohesion and cooperation. But no research has manipulated these variables in the field to investigate their causal - and potentially interactive - effects on prosocial behaviour. Across four experimental sessions involving large samples of strangers, we manipulated the synchronous and physiologically arousing affordances of a group marching task within a sports stadium. We observed participants' subsequent movement, grouping, and cooperation via a camera hidden in the stadium's roof. Synchrony and arousal both showed main effects, predicting larger groups, tighter clustering, and more cooperative behaviour in a free-rider dilemma. Synchrony and arousal also interacted on measures of clustering and cooperation such that synchrony only encouraged closer clustering - and encouraged greater cooperation - when paired with physiological arousal. The research helps us understand why synchrony and arousal often co-occur in rituals around the world. It also represents the first use of real-time spatial tracking as a precise and naturalistic method of simulating collective rituals.},
author = {Jackson, J.C. and Jong, J. and Bilkey, D. and Whitehouse, H. and Zollmann, S. and McNaughton, C. and Halberstadt, J.},
doi = {10.1038/s41598-017-18023-4},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
title = {{Synchrony and Physiological Arousal Increase Cohesion and Cooperation in Large Naturalistic Groups}},
volume = {8},
year = {2018}
}
@inproceedings{Langlotz2018a,
abstract = {Given the advancements in ubiquitous computing we can nowadays link information to places and objects anywhere on the globe. For years, map-based interfaces have been the primary interfaces to browse and retrieve this situated media. While more recently also other interface concepts for situated media have found their way out of the research labs, most notably Augmented Reality, this work looks into a concept that has widely been ignored: Accurate pointing in outdoor environments. This works presents Urban Pointer a phone-based pointing interface that utilizes computer vision to enable accurate pointing in urban environments together with some first insights on the implementation.},
author = {Langlotz, Tobias and Tappeiner, Elias and Zollmann, Stefanie and Ventura, Jonathan and Regenbrecht, Holger},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/3170427.3188565},
isbn = {9781450356206},
keywords = {Augmented Reality,Location-based systems,Mobile Interfaces,Pointing,Situated Media,Tricorder},
title = {{Urban pointing: Browsing situated media using accurate pointing interfaces}},
year = {2018}
}
@article{Grubert2017,
abstract = {Augmented Reality is a technique that enables users to interact with their physical environment through the overlay of digital information. While being researched for decades, more recently, Augmented Reality moved out of the research labs and into the field. While most of the applications are used sporadically and for one particular task only, current and future scenarios will provide a continuous and multi-purpose user experience. Therefore, in this paper, we present the concept of Pervasive Augmented Reality, aiming to provide such an experience by sensing the user's current context and adapting the AR system based on the changing requirements and constraints. We present a taxonomy for Pervasive Augmented Reality and context-aware Augmented Reality, which classifies context sources and context targets relevant for implementing such a context-aware, continuous Augmented Reality experience. We further summarize existing approaches that contribute towards Pervasive Augmented Reality. Based our taxonomy and survey, we identify challenges for future research directions in Pervasive Augmented Reality.},
author = {Grubert, Jens and Langlotz, Tobias and Zollmann, Stefanie and Regenbrecht, Holger},
doi = {10.1109/TVCG.2016.2543720},
isbn = {1077-2626},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {6},
pages = {1706--1724},
pmid = {27008668},
title = {{Towards Pervasive Augmented Reality: Context-Awareness in Augmented Reality}},
url = {http://ieeexplore.ieee.org/document/7435333/},
volume = {23},
year = {2017}
}
@inproceedings{Zollmann2016,
author = {Zollmann, Stefanie and Poglitsch, Christian and Ventura, Jonathan},
booktitle = {2016 International Conference on Image and Vision Computing New Zealand (IVCNZ)},
doi = {10.1109/IVCNZ.2016.7804440},
isbn = {978-1-5090-2748-4},
month = {nov},
pages = {1--6},
publisher = {IEEE},
title = {{VISGIS: Dynamic situated visualization for geographic information systems}},
url = {http://ieeexplore.ieee.org/document/7804440/},
year = {2016}
}
@article{Zollmann2014a,
author = {Zollmann, Stefanie and Hoppe, Christof and Kluckner, Stefan and Bischof, Horst and Reitmayr, Gerhard},
journal = {accepted for Proceedings of the IEEE},
title = {{Augmented Reality for Construction Site Monitoring and Documentation}},
year = {2014}
}
@article{Zollmann2014,
abstract = {Micro aerial vehicles equipped with high-resolution cameras can be used to create aerial reconstructions of an area of interest. In that context automatic flight path planning and autonomous flying is often applied but so far cannot fully replace the human in the loop, supervising the flight on-site to assure that there are no collisions with obstacles. Unfortunately, this workflow yields several issues, such as the need to mentally transfer the aerial vehicle & amp;#146;s position between 2D map positions and the physical environment, and the complicated depth perception of objects flying in the distance. Augmented Reality can address these issues by bringing the flight planning process on-site and visualizing the spatial relationship between the planned or current positions of the vehicle and the physical environment. In this paper, we present Augmented Reality supported navigation and flight planning of micro aerial vehicles by augmenting the user's view with relevant information for flight planning and live feedback for flight supervision. Furthermore, we introduce additional depth hints supporting the user in understanding the spatial relationship of virtual waypoints in the physical world and investigate the effect of these visualization techniques on the spatial understanding. {\textcopyright} 2014 IEEE.},
author = {Zollmann, S. and Hoppe, C. and Langlotz, T. and Reitmayr, G.},
doi = {10.1109/TVCG.2014.24},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Augmented reality,Micro aerial vehicles,Visualization},
number = {4},
title = {{FlyAR: Augmented reality supported micro aerial vehicle navigation}},
volume = {20},
year = {2014}
}
@inproceedings{Zollmann:2014:IXV:2686612.2686642,
address = {New York, NY, USA},
author = {Zollmann, Stefanie and Grasset, Raphael and Reitmayr, Gerhard and Langlotz, Tobias},
booktitle = {Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design},
doi = {10.1145/2686612.2686642},
isbn = {978-1-4503-0653-9},
keywords = {X-ray,augmented reality,evaluation,visualization},
pages = {194--203},
publisher = {ACM},
series = {OzCHI '14},
title = {{Image-based X-ray Visualization Techniques for Spatial Understanding in Outdoor Augmented Reality}},
url = {http://doi.acm.org/10.1145/2686612.2686642},
year = {2014}
}
@inproceedings{Zollmann2014b,
abstract = {Copyright 2014 ACM. This paper evaluates different state-of-the-art approaches for implementing an X-ray view in Augmented Reality (AR). Our focus is on approaches supporting a better scene understanding and in particular a better sense of depth order between physical objects and digital objects. One of the main goals of this work is to provide effective X-ray visualization techniques that work in unprepared outdoor environments. In order to achieve this goal, we focus on methods that automatically extract depth cues from video images. The extracted depth cues are combined in ghosting maps that are used to assign each video image pixel a transparency value to control the overlay in the AR view. Within our study, we analyze three different types of ghosting maps, 1) alpha-blending which uses a uniform alpha value within the ghosting map, 2) edge-based ghosting which is based on edge extraction and 3) image-based ghosting which incorporates perceptual grouping, saliency information, edges and texture details. Our study results demonstrate that the latter technique helps the user to understand the subsurface location of virtual objects better than using alpha-blending or the edge-based ghosting.},
author = {Zollmann, S. and Grasset, R. and Reitmayr, G. and Langlotz, T.},
booktitle = {Proceedings of the 26th Australian Computer-Human Interaction Conference, OzCHI 2014},
doi = {10.1145/2686612.2686642},
isbn = {9781450306539},
keywords = {Augmented Reality,Evaluation,Visualization,X-ray},
title = {{Image-based X-ray visualization techniques for spatial understanding in outdoor augmented reality}},
year = {2014}
}
@inproceedings{Kalkofen2013,
author = {Kalkofen, Denis and Veas, Eduardo and Zollmann, Stefanie and Steinberger, Markus and Schmalstieg, Dieter},
booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2013)},
title = {{Adaptive Ghosted Views for Augmented Reality}},
year = {2013}
}
@inproceedings{Langlotz2013,
abstract = {This paper describes spatially aligned user-generated audio annotations and the integration with visual augmentations into a single mobile AR system. Details of our prototype system are presented, along with an explorative usability study and technical evaluation of the design. Mobile Augmented Reality applications allow for visual augmentations as well as tagging and annotation of the surrounding environment. Texts and graphics are currently the media of choice for these applications with GPS coordinates used to determine spatial location. Our research demonstrates that the use of visually guided audio annotations that are positioned and orientated in augmented outdoor space successfully provides for additional, novel, and enhanced mobile user experience.},
author = {Langlotz, T. and Regenbrecht, H. and Zollmann, S. and Schmalstieg, D.},
booktitle = {Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration, OzCHI 2013},
doi = {10.1145/2541016.2541022},
isbn = {9781450325257},
keywords = {Augmented Reality,Mobile phone,Spatial Audio},
title = {{Audio stickies: Visually-guided spatial audio annotations on a mobile augmented reality platform}},
year = {2013}
}
@article{Zollmann2012b,
author = {Zollmann, Stefanie and Schall, Gerhard and Junghanns, Sebastian and Reitmayr, Gerhard},
doi = {10.1007/978-3-642-33179-4_64},
isbn = {978-3-642-33179-4},
journal = {Advances in Visual Computing},
keywords = {Augmented Reality,GIS,visualization},
pages = {675--685},
title = {{Comprehensible and Interactive Visualizations of GIS Data in Augmented Reality}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33179-4_64},
year = {2012}
}
@inproceedings{Zollmann2012,
author = {Zollmann, Stefanie and Reitmayr, Gerhard},
booktitle = {Proceedings of the 18th ACM symposium on Virtual reality software and technology},
doi = {10.1145/2407336.2407347},
isbn = {978-1-4503-1469-5},
keywords = {augmented reality,depth map estimation,segmentation},
month = {dec},
pages = {53--60},
title = {{Dense depth maps from sparse models and image coherence for augmented reality}},
url = {http://dl.acm.org/citation.cfm?id=2407336.2407347},
year = {2012}
}
@inproceedings{Kluckner2012,
author = {Kluckner, Stefan and Hatzl, J. and Klopschitz, Manfred and Morard, J. and Hoppe, Christof and Zollmann, Stefanie and Bischof, Horst and Reitmayr, Gerhard},
booktitle = {DAGM, Workshop Computer Vision in Applications},
title = {{Image-based As-Built Site Documentation and Analysis - Applications and Challenges}},
year = {2012}
}
@inproceedings{Zollmann2012a,
abstract = {In this paper we present an approach for visualizing time-oriented data of dynamic scenes in an on-site AR view. Visualizations of time-oriented data have special challenges compared to the visualization of arbitrary virtual objects. Usually, the 4D data occludes a large part of the real scene. Additionally, the data sets from different points in time may occlude each other. Thus, it is important to design adequate visualization techniques that provide a comprehensible visualization. In this paper we introduce a visualization concept that uses overview and detail techniques to present 4D data in different detail levels. These levels provide at first an overview of the 4D scene, at second information about the 4D change of a single object and at third detailed information about object appearance and geometry for specific points in time. Combining the three levels of detail with interactive transitions such as magic lenses or distorted viewing techniques enables the user to understand the relationship between them. Finally we show how to apply this concept for construction site documentation and monitoring.},
author = {Zollmann, Stefanie and Kalkofen, Denis and Hoppe, Christof and Kluckner, Stefan and Bischof, Horst and Reitmayr, Gerhard},
booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2012)},
doi = {10.1109/ISMAR.2012.6402554},
keywords = {4D,Augmented Reality,Visualization,time-oriented},
title = {{Interactive 4D overview and detail visualization in augmented reality}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6402554&contentType=Conference+Publications&queryText=Interactive+4D+Overview+and+Detail+Visualization+in+Augmented+Reality},
year = {2012}
}
@inproceedings{Hoppe2012,
address = {Mala Nedelja, Slovenia},
author = {Hoppe, Christof and Wendel, Andreas and Zollmann, Stefanie and Pirker, Katrin and Irschara, Arnold and Bischof, Horst and Kluckner, Stefan},
booktitle = {Computer Vision Winter Workshop},
title = {{Photogrammetric Camera Network Design for Micro Aerial Vehicles}},
year = {2012}
}
@article{Schall2012,
author = {Schall, Gerhard and Zollmann, Stefanie and Reitmayr, Gerhard},
doi = {10.1007/s00779-012-0599-x},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {3d gis {\'{a}},and context-aware,computing {\'{a}} surveying,geospatial interaction {\'{a}} location-,mobile augmented reality {\'{a}}},
month = {sep},
number = {7},
pages = {1533--1549},
title = {{Smart Vidente: advances in mobile augmented reality for interactive visualization of underground infrastructure}},
url = {http://link.springer.com/10.1007/s00779-012-0599-x},
volume = {17},
year = {2012}
}
@article{Kluckner2011a,
author = {Kluckner, Stefan and Birchbauer, Josef-alois and Windisch, Claudia and Hoppe, Christof and Irschara, Arnold and Wendel, Andreas and Zollmann, Stefanie and Reitmayr, Gerhard and Bischof, Horst},
isbn = {9781457708459},
journal = {Computer},
pages = {531--532},
title = {{AVSS 2011 demo session : Construction Site Monitoring from Highly-Overlapping MAV images}},
year = {2011}
}
@inproceedings{Kluckner2011b,
abstract = {We present a concept for automatic construction site monitoring by taking into account 4D information (3D over time), that is acquired from highly-overlapping digital aerial images. On the one hand today's maturity of flying micro aerial vehicles (MAVs) enables a low-cost and an efficient image acquisition of high-quality data that maps construction sites entirely from many varying viewpoints. On the other hand, due to low-noise sensors and high redundancy in the image data, recent developments in 3D reconstruction workflows have benefited the automatic computation of accurate and dense 3D scene information. Having both an inexpensive high-quality image acquisition and an efficient 3D analysis workflow enables monitoring, documentation and visualization of observed sites over time with short intervals. Relating acquired 4D site observations, composed of color, texture, geometry over time, largely supports automated methods toward full scene understanding, the acquisition of both the change and the construction site's progress. {\textcopyright} 2011 IEEE.},
author = {Kluckner, S. and Birchbauer, J.-A. and Windisch, C. and Hoppe, C. and Irschara, A. and Wendel, A. and Zollmann, S. and Reitmayr, G. and Bischof, H.},
booktitle = {2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2011},
doi = {10.1109/AVSS.2011.6027402},
isbn = {9781457708459},
title = {{AVSS 2011 demo session: Construction site monitoring from highly-overlapping MAV images}},
year = {2011}
}
@inproceedings{Schall2011,
address = {Stockholm},
author = {Schall, Gerhard and Zollman, Stefanie and Reitmayr, Gerhard},
booktitle = {Mobile HCI 2011 Workshop "Mobile Work Efficiency: Enhancing Workflows with Mobile Devices"},
isbn = {9781450305419},
keywords = {3d gis,acm classification keywords,computing,data,geospatial,location and context aware,mobile devices,outdoor augmented reality},
pages = {1--4},
title = {{Bridging the gap between Planning and Surveying with Augmented Reality User Interfaces}},
year = {2011}
}
@inproceedings{Zollmann2011,
address = {Basel},
author = {Zollmann, S. and Reitmayr, G.},
booktitle = {ISMAR Workshop on Visualization in Mixed Reality Environments},
title = {{Combining sparse models and image coherence for supporting advanced visualization techniques in augmented reality}},
year = {2011}
}
@inproceedings{Kluckner2011,
author = {Kluckner, Stefan and Birchbauer, Josef A and Windisch, Claudia and Hoppe, Christof and Irschara, Arnold and Wendel, Andreas and Zollmann, Stefanie and Reitmayr, Gerhard and Bischof, Horst},
booktitle = {Proceedings of the IEEE International Conference on Advanced Video and Signalbased Surveillance AVSS Industrial Session},
title = {{Construction Site Monitoring from Highly-Overlapping MAV Images}},
year = {2011}
}
@inproceedings{Steiner2011,
author = {Steiner, Jochen and Zollmann, Stefanie and Reitmayr, Gerhard},
booktitle = {Computer Vision Winter Workshop},
title = {{Incremental Superpixels for Real-Time Video Analysis}},
url = {http://cvww2011.icg.tugraz.at/papers_web/p20.pdf},
year = {2011}
}
@article{Langlotz2011,
abstract = {We present a novel system allowing in situ content creation for mobile Augmented Reality in unprepared environments. This system targets smartphones and therefore allows a spontaneous authoring while in place. We describe two different scenarios, which are depending on the size of the working environment and consequently use different tracking techniques. A natural feature-based approach for planar targets is used for small working spaces whereas for larger working environments, such as in outdoor scenarios, a panoramic-based orientation tracking is deployed. Both are integrated into one system allowing the user to use the same interaction for creating the content applying a set of simple, yet powerful modeling functions for content creation. The resulting content for Augmented Reality can be shared with other users using a dedicated content server or kept in a private inventory for later use.},
author = {Langlotz, Tobias and Mooslechner, Stefan and Zollmann, Stefanie and Degendorfer, Claus and Reitmayr, Gerhard and Schmalstieg, Dieter},
doi = {10.1007/s00779-011-0430-0},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {Computer Science},
month = {jul},
pages = {1--8},
publisher = {Springer London},
title = {{Sketching up the world: in situ authoring for mobile Augmented Reality}},
url = {http://www.springerlink.com/content/g270j64762108654/},
year = {2011}
}
@inproceedings{Zollmann2010,
abstract = {In augmented reality displays, X-Ray visualization techniques make hidden objects visible through combining the physical view with an artificial rendering of the hidden information. An important step in X-Ray visualization is to decide which parts of the physical scene should be kept and which should be replaced by overlays. The combination should provide users with essential perceptual cues to understand the relationship of depth between hidden information and the physical scene. In this paper we present an approach that addresses this decision in unknown environments by analyzing camera images of the physical scene and using the extracted information for occlusion management. Pixels are grouped into perceptually coherent image regions and a set of parameters is determined for each region. The parameters change the X-Ray visualization for either preserving existing structures or generating synthetic structures. Finally, users can customize the overall opacity of foreground regions to adapt the visualization.},
author = {Zollmann, Stefanie and Kalkofen, Denis and Mendez, Erick and Reitmayr, Gerhard},
booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2010)},
doi = {10.1109/ISMAR.2010.5643546},
isbn = {978-1-4244-9343-2},
keywords = {Augmented Reality,Ghostings,Visualization,X-Ray},
month = {oct},
pages = {19--26},
publisher = {IEEE},
title = {{Image-based ghostings for single layer occlusions in augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5643546},
year = {2010}
}
@inproceedings{Gruber2010,
author = {Gruber, Lukas and Zollmann, Stefanie and Wagner, Daniel and Schmalstieg, Dieter and Hollerer, Tobias},
booktitle = {2010 20th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2010.880},
isbn = {978-1-4244-7542-1},
keywords = {Natural feature tracking,simulation,tracking target optimization},
month = {aug},
pages = {3607--3610},
publisher = {IEEE},
title = {{Optimization of Target Objects for Natural Feature Tracking}},
url = {http://portal.acm.org/citation.cfm?id=1904935.1905612},
year = {2010}
}
@inproceedings{Gruber2010a,
abstract = {This paper investigates possible physical alterations of tracking targets to obtain improved 6DoF pose detection for a camera observing the known targets. We explore the influence of several texture characteristics on the pose detection, by simulating a large number of different target objects and camera poses. Based on statistical observations, we rank the importance of characteristics such as texturedness and feature distribution for a specific implementation of a 6DoF tracking technique. These findings allow informed modification strategies for improving the tracking target objects themselves, in the common case of man-made targets, as for example used in advertising. This fundamentally differs from and complements the traditional approach of leaving the targets unchanged while trying to optimize the tracking algorithms and parameters. {\textcopyright} 2010 IEEE.},
author = {Gruber, L. and Zollman, S. and Wagner, D. and Schmalstieg, D. and H{\"{o}}llerer, T.},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2010.880},
isbn = {9780769541099},
issn = {10514651},
keywords = {Natural feature tracking,Simulation,Tracking target optimization},
title = {{Optimization of target objects for natural feature tracking}},
year = {2010}
}
@inproceedings{Langlotz2010a,
address = {Gwangju, South Korea},
author = {Langlotz, Tobias and Mooslechner, Stefan and Zollmann, Stefanie and Degendorfer, Claus and Reitmayr, Gerhard and Schmalstieg, Dieter},
booktitle = {International Workshop on Smartphone Applications and Services 2010},
title = {{Sketching up the world: In-situ authoring for mobile Augmented Reality}},
year = {2010}
}
@inproceedings{Gruber2010b,
abstract = {We describe the design and implementation of a physical and virtual model of an imaginary urban scene - the "City of Sights" - that can serve as a backdrop or "stage" for a variety of Augmented Reality (AR) research. We argue that the AR research community would benefit from such a standard model dataset which can be used for evaluation of such AR topics as tracking systems, modeling, spatial AR, rendering tests, collaborative AR and user interface design. By openly sharing the digital blueprints and assembly instructions for our models, we allow the proposed set to be physically replicable by anyone and permit customization and experimental changes to the stage design which enable comprehensive exploration of algorithms and methods. Furthermore we provide an accompanying rich dataset consisting of video sequences under varying conditions with ground truth camera pose. We employed three different ground truth acquisition methods to support a broad range of use cases. The goal of our design is to enable and improve the replicability and evaluation of future augmented reality research. {\textcopyright}2010 IEEE.},
author = {Gruber, L. and Gauglitz, S. and Ventura, J. and Zollmann, S. and Huber, M. and Schlegel, M. and Klinker, G. and Schmalstieg, D. and H{\"{o}}llerer, T.},
booktitle = {9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings},
doi = {10.1109/ISMAR.2010.5643564},
isbn = {9781424493449},
title = {{The city of sights: Design, construction, and measurement of an augmented reality stage set}},
year = {2010}
}
@inproceedings{Kalkofen,
author = {Kalkofen, Denis and Zollman, Stefanie and Schall, Gerhard and Reitmayr, Gerhard and Schmalstieg, Dieter},
booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2009)},
title = {{Adaptive Visualization in Outdoor AR Displays}},
year = {2009}
}
@inproceedings{Gruber2009,
author = {Gruber, Lukas and Zollmann, Stefanie and Wagner, Daniel and Schmalstieg, Dieter},
booktitle = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2009.5336469},
isbn = {978-1-4244-5390-0},
month = {oct},
pages = {189--190},
publisher = {IEEE},
title = {{Evaluating the trackability of natural feature-point sets}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/ISMAR.2009.5336469},
year = {2009}
}
@inproceedings{Zollmann2009,
author = {Zollmann, Stefanie and Langlotz, Tobias},
booktitle = {2009 IEEE Symposium on 3D User Interfaces},
doi = {10.1109/3DUI.2009.4811229},
isbn = {978-1-4244-3965-2},
keywords = {projector-based interaction,projector-based visualization,spatial augmented reality},
month = {mar},
pages = {147--148},
publisher = {Ieee},
title = {{Poster: Spatially augmented tape drawing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4811229},
year = {2009}
}
@inproceedings{Zollmann2007a,
author = {Zollmann, Stefanie and Bimber, Oliver},
booktitle = {Proceedings of EUROGRAPHICS 2007},
title = {{Imperceptible Calibration for Radiometric Compensation}},
year = {2007}
}
@article{Zollmann2007,
author = {Zollmann, S. and Langlotz, T. and Bimber, O.},
journal = {Journal of Virtual Reality and Broadcasting},
keywords = {dynamic geometric calibration,optical flow,projector-camera systems},
number = {6},
title = {{Passive-Active Geometric Calibration for View-Dependent Projections onto Arbitrary Surfaces}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.3856&rep=rep1&type=pdf},
volume = {4},
year = {2007}
}
@article{Bimber2006a,
author = {Bimber, Oliver and Grundh{\"{o}}fer, Anselm and Zollmann, Stefanie and Kolster, Daniel},
journal = {Journal of Virtual Reality and Broadcasting},
title = {{Digital Illumination for Augmented Studios}},
year = {2006}
}
@inproceedings{Zollmann2006,
author = {Zollmann, S. and Langlotz, T. and Bimber, O.},
booktitle = {Workshop on Virtual and Augmented Reality of the GI-Fachgruppe AR/VR 2006},
keywords = {dynamic geometric calibration,optical flow,projector-camera systems},
number = {6},
title = {{Passive-active geometric calibration for view-dependent projections onto arbitrary surfaces}},
volume = {4},
year = {2006}
}
@article{Bimber2005,
address = {New York, New York, USA},
author = {Bimber, Oliver and Coriand, Franz and Kleppe, Alexander and Bruns, Erich and Zollmann, Stefanie and Langlotz, Tobias},
doi = {10.1145/1198555.1198716},
journal = {ACM SIGGRAPH 2005 Courses on - SIGGRAPH '05},
pages = {6},
publisher = {ACM Press},
title = {{Superimposing pictorial artwork with projected imagery}},
url = {http://portal.acm.org/citation.cfm?doid=1198555.1198716},
year = {2005}
}
@article{Bimber2005d,
author = {Bimber, O. and Coriand, F. and Kleppe, A. and Bruns, E. and Zollmann, S. and Langlotz, T.},
doi = {10.1109/MMUL.2005.9},
issn = {1070-986X},
journal = {IEEE Multimedia},
month = {jan},
number = {1},
pages = {16--26},
title = {{Superimposing pictorial artwork with projected imagery}},
url = {http://dl.acm.org/citation.cfm?id=1042196.1042326},
volume = {12},
year = {2005}
}

  #code = {https://github.com/LMescheder/Occupancy-Networks},
  #img            = {assets/img/publications/onet.jpg},
  #pdf            = {http://www.cvlibs.net/publications/Mescheder2019CVPR.pdf},
  #supp           = {http://www.cvlibs.net/publications/Mescheder2019CVPR_supplementary.pdf},
  #video          = {http://www.youtube.com/watch?v=w1Qo3bOiPaE&t=6s&vq=hd1080&autoplay=1},
  #poster = {http://www.cvlibs.net/publications/Mescheder2019CVPR_poster.pdf},
  #html = {https://avg.is.mpg.de/publications/occupancy-networks},
  #award = {Oral Presentation, Best Paper Finalist},
#}
